{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, Math, Latex\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Loading\n",
    "data = pd.read_csv('exams.csv') #read from dataset with columns exam_1, exam_2 and admitted - shows passed or not (0 or 1)\n",
    "data.head()\n",
    "#min-max normalization\n",
    "ndata = data.copy() #normalized form of data set\n",
    "#normalizing first two columns, third column only contains 0 and 1s\n",
    "\n",
    "ndata['exam_1'] = (ndata['exam_1']-ndata['exam_1'].min())/(ndata['exam_1'].max()-ndata['exam_1'].min())\n",
    "ndata['exam_2'] = (ndata['exam_2']-ndata['exam_2'].min())/(ndata['exam_2'].max()-ndata['exam_2'].min())\n",
    "ndata['admitted'] = ndata['admitted']\n",
    "ndata.head()\n",
    "\n",
    "\n",
    "#Sigmoid function.\n",
    "\n",
    "def sigmoidFunction(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    \n",
    "#cost function\n",
    "\n",
    "def costFunction(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() \n",
    "\n",
    "\n",
    "#Gradient descent implementation from scratch.\n",
    "\n",
    "x = ndata[['exam_1', 'exam_2']].values #here data for exam1 and exam2 is included to x\n",
    "y = ndata['admitted'].values #here data for admission (0,1) included to y\n",
    "\n",
    "#these initializations randomly chosen (can be changed)\n",
    "alpha = 0.01\n",
    "numOfIterations = 100000\n",
    "\n",
    "#initializations \n",
    "x = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n",
    "theta = np.zeros(x.shape[1])\n",
    "allCosts = []\n",
    "\n",
    "for i in range(numOfIterations):\n",
    "    h = sigmoidFunction(np.dot(x, theta))\n",
    "    error = h - y\n",
    "    gradient = np.dot(x.T, (error)) / y.size\n",
    "    theta -= alpha * gradient\n",
    "    if i % 1000 == 0: print(\"loss:\", costFunction(h, y))\n",
    "    allCosts.append(costFunction(h, y))\n",
    "    \n",
    "    \n",
    "    \n",
    "plt.figure(figsize = (10,5))\n",
    "plt.title('First exam score vs Second exam score.')\n",
    "plt.xlabel('First exam')\n",
    "plt.ylabel('Second exam')\n",
    "#nearly the same graph as shown in visualization part 1 but here with decision boundary\n",
    "#admitted student points shown in green\n",
    "plt.scatter(ndata[ndata['admitted'] == 1]['exam_1'], ndata[ndata['admitted'] == 1]['exam_2'], color = 'green') \n",
    "#failed student points shown in red\n",
    "plt.scatter(ndata[ndata['admitted'] == 0]['exam_1'], ndata[ndata['admitted'] == 0]['exam_2'], color = 'red')\n",
    "#boundary datas\n",
    "bx = [np.min(ndata['exam_1']), np.max(ndata['exam_1'])]\n",
    "by = - (theta[0] + np.dot(theta[1], bx)) / theta[2]\n",
    "plt.plot(bx, by) #decision boundary\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
